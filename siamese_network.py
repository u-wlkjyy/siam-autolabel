import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import os
import glob
from PIL import Image
import numpy as np
import random

class SiameseDataset(Dataset):
    """å­ªç”Ÿç½‘ç»œæ•°æ®é›†ç±»"""
    
    def __init__(self, dataset_path: str, transform=None, mode='train', train_ratio=0.8):
        """
        Args:
            dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            transform: å›¾åƒå˜æ¢
            mode: 'train' æˆ– 'test' æˆ– 'val'
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        """
        self.dataset_path = dataset_path
        self.transform = transform
        self.mode = mode
        
        # èŽ·å–æ‰€æœ‰æ–‡ä»¶å¤¹
        self.folders = [f for f in os.listdir(dataset_path) 
                       if os.path.isdir(os.path.join(dataset_path, f))]
        
        # æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†
        random.shuffle(self.folders)
        split_idx = int(len(self.folders) * train_ratio)
        
        if mode == 'train':
            self.folders = self.folders[:split_idx]
        elif mode == 'test':
            self.folders = self.folders[split_idx:]
        elif mode == 'val':
            # ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
            val_split = int(split_idx * 0.8)
            self.folders = self.folders[val_split:split_idx]
        
        print(f"{mode.upper()}é›†åŒ…å« {len(self.folders)} ä¸ªæ ·æœ¬æ–‡ä»¶å¤¹")
        
    def __len__(self):
        # æ¯ä¸ªæ–‡ä»¶å¤¹ç”Ÿæˆå¤šä¸ªæ­£è´Ÿæ ·æœ¬å¯¹
        return len(self.folders) * 20  # æ¯ä¸ªæ–‡ä»¶å¤¹ç”Ÿæˆ20ä¸ªæ ·æœ¬å¯¹
    
    def __getitem__(self, idx):
        """ç”Ÿæˆä¸€ä¸ªæ ·æœ¬å¯¹å’Œæ ‡ç­¾"""
        folder_idx = idx // 20
        folder_name = self.folders[folder_idx]
        folder_path = os.path.join(self.dataset_path, folder_name)
        
        # è¯»å–é—®é¢˜å›¾ç‰‡
        question_path = os.path.join(folder_path, 'question.png')
        if not os.path.exists(question_path):
            # å¦‚æžœæ²¡æœ‰question.pngï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªå›¾ç‰‡ä½œä¸ºanchor
            all_images = glob.glob(os.path.join(folder_path, '*.png'))
            question_path = random.choice(all_images)
        
        question_img = Image.open(question_path).convert('RGB')
        
        # èŽ·å–æ­£æ ·æœ¬ï¼ˆç­”æ¡ˆï¼‰å’Œè´Ÿæ ·æœ¬
        answer_files = glob.glob(os.path.join(folder_path, 'geetest_answer_*.png'))
        negative_files = glob.glob(os.path.join(folder_path, 'geetest_[0-9].png'))
        
        # éšæœºå†³å®šç”Ÿæˆæ­£æ ·æœ¬å¯¹è¿˜æ˜¯è´Ÿæ ·æœ¬å¯¹
        is_positive = random.random() > 0.5
        
        if is_positive and answer_files:
            # ç”Ÿæˆæ­£æ ·æœ¬å¯¹
            pair_path = random.choice(answer_files)
            label = 1
        elif negative_files:
            # ç”Ÿæˆè´Ÿæ ·æœ¬å¯¹
            pair_path = random.choice(negative_files)
            label = 0
        else:
            # å¦‚æžœæ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œéšæœºé€‰æ‹©
            all_images = glob.glob(os.path.join(folder_path, '*.png'))
            all_images = [img for img in all_images if img != question_path]
            if all_images:
                pair_path = random.choice(all_images)
                label = 1 if 'answer' in pair_path else 0
            else:
                # æœ€åŽçš„fallback
                pair_path = question_path
                label = 1
        
        pair_img = Image.open(pair_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            question_img = self.transform(question_img)
            pair_img = self.transform(pair_img)
        
        return question_img, pair_img, torch.tensor(label, dtype=torch.float32)


class ResNetBackbone(nn.Module):
    """ResNetç‰¹å¾æå–å™¨"""
    
    def __init__(self, pretrained=True, feature_dim=512):
        super(ResNetBackbone, self).__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet-18
        self.resnet = models.resnet18(pretrained=pretrained)
        
        # ç§»é™¤æœ€åŽçš„åˆ†ç±»å±‚
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])
        
        # æ·»åŠ ç‰¹å¾é™ç»´å±‚
        self.feature_projector = nn.Sequential(
            nn.Linear(512, feature_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(feature_dim, feature_dim)
        )
        
    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.resnet(x)
        features = features.view(features.size(0), -1)  # å±•å¹³
        
        # ç‰¹å¾æŠ•å½±
        features = self.feature_projector(features)
        
        # L2å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        return features


class SiameseNetwork(nn.Module):
    """å­ªç”Ÿç¥žç»ç½‘ç»œ"""
    
    def __init__(self, feature_dim=512):
        super(SiameseNetwork, self).__init__()
        
        # å…±äº«çš„ç‰¹å¾æå–å™¨
        self.backbone = ResNetBackbone(pretrained=True, feature_dim=feature_dim)
        
        # ç›¸ä¼¼åº¦è®¡ç®—å±‚
        self.similarity_head = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
    def forward(self, img1, img2):
        # æå–ä¸¤ä¸ªå›¾åƒçš„ç‰¹å¾
        feat1 = self.backbone(img1)
        feat2 = self.backbone(img2)
        
        # è®¡ç®—ç‰¹å¾è·ç¦»ï¼ˆå¯é€‰æ–¹æ³•ï¼‰
        euclidean_distance = F.pairwise_distance(feat1, feat2)
        cosine_similarity = F.cosine_similarity(feat1, feat2)
        
        # è¿žæŽ¥ç‰¹å¾è¿›è¡Œç›¸ä¼¼åº¦é¢„æµ‹
        combined_features = torch.cat([feat1, feat2], dim=1)
        similarity_score = self.similarity_head(combined_features)
        
        return similarity_score.squeeze(), euclidean_distance, cosine_similarity


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, distance, label):
        # å¯¹æ¯”æŸå¤±è®¡ç®—
        loss = torch.mean(
            label * torch.pow(distance, 2) + 
            (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)
        )
        return loss


class TripletLoss(nn.Module):
    """ä¸‰å…ƒç»„æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=0.3):
        super(TripletLoss, self).__init__()
        self.margin = margin
        
    def forward(self, anchor, positive, negative):
        distance_positive = F.pairwise_distance(anchor, positive)
        distance_negative = F.pairwise_distance(anchor, negative)
        
        loss = torch.mean(torch.clamp(
            distance_positive - distance_negative + self.margin, min=0.0))
        
        return loss


def get_transforms():
    """èŽ·å–æ•°æ®å˜æ¢"""
    
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.2),
        transforms.RandomRotation(degrees=5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, test_transform


def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):
    """è®­ç»ƒæ¨¡åž‹"""
    import time
    import sys
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)
    
    # å¤šç§æŸå¤±å‡½æ•°
    bce_loss = nn.BCELoss()
    contrastive_loss = ContrastiveLoss(margin=1.0)
    
    best_val_acc = 0.0
    train_losses = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        running_loss = 0.0
        start_time = time.time()
        
        for batch_idx, (img1, img2, labels) in enumerate(train_loader):
            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            similarity_scores, euclidean_dist, cosine_sim = model(img1, img2)
            
            # è®¡ç®—æŸå¤±ï¼ˆç»“åˆå¤šç§æŸå¤±ï¼‰
            bce_loss_val = bce_loss(similarity_scores, labels)
            contrastive_loss_val = contrastive_loss(euclidean_dist, labels)
            
            # æ€»æŸå¤±
            total_loss_val = bce_loss_val + 0.5 * contrastive_loss_val
            
            # åå‘ä¼ æ’­
            total_loss_val.backward()
            optimizer.step()
            
            # æ›´æ–°æŸå¤±ç»Ÿè®¡
            total_loss += total_loss_val.item()
            running_loss += total_loss_val.item()
            
            # è®¡ç®—è¿›åº¦
            progress = (batch_idx + 1) / len(train_loader)
            bar_length = 40
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + 'â–’' * (bar_length - filled_length)
            
            # è®¡ç®—é€Ÿåº¦å’ŒETA
            elapsed_time = time.time() - start_time
            batches_per_sec = (batch_idx + 1) / elapsed_time if elapsed_time > 0 else 0
            eta_seconds = (len(train_loader) - batch_idx - 1) / batches_per_sec if batches_per_sec > 0 else 0
            eta_str = f"{int(eta_seconds//60):02d}:{int(eta_seconds%60):02d}"

            # æ˜¾ç¤ºè¿›åº¦
            avg_loss = running_loss / (batch_idx + 1)
            sys.stdout.write(f'\rEpoch {epoch+1}/{num_epochs} |{bar}| '
                           f'{batch_idx+1}/{len(train_loader)} '
                           f'[{progress*100:5.1f}%] '
                           f'loss: {avg_loss:.4f} '
                           f'ETA: {eta_str} '
                           f'{batches_per_sec:.1f}it/s')
            sys.stdout.flush()
        
        # æ¢è¡Œå¹¶æ˜¾ç¤ºepochæ€»ç»“
        print()
        avg_train_loss = total_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        print("Validating...")
        val_acc = evaluate_model(model, val_loader, device, verbose=False)  # éªŒè¯æ—¶ä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        val_accuracies.append(val_acc)
        
        # epochæ€»ç»“
        epoch_time = time.time() - start_time
        lr = optimizer.param_groups[0]['lr']
        print(f'Epoch {epoch+1}/{num_epochs}: '
              f'train_loss={avg_train_loss:.4f} '
              f'val_acc={val_acc:.4f} '
              f'lr={lr:.6f} '
              f'time={epoch_time:.1f}s')
        
        # ä¿å­˜æœ€ä½³æ¨¡åž‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_siamese_model.pth')
            print(f'ðŸ† New best model saved! Val Acc: {best_val_acc:.4f}')
        
        scheduler.step()
        print("-" * 80)
    
    return train_losses, val_accuracies


def evaluate_model(model, test_loader, device='cuda', verbose=True):
    """è¯„ä¼°æ¨¡åž‹"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for img1, img2, labels in test_loader:
            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)
            
            similarity_scores, _, _ = model(img1, img2)
            predictions = (similarity_scores > 0.5).float()
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—åŸºæœ¬å‡†ç¡®çŽ‡
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    accuracy = np.mean(all_predictions == all_labels)
    
    if verbose:
            print(f'å‡†ç¡®çŽ‡: {accuracy:.4f}')
            # æ‰‹åŠ¨è®¡ç®—ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡
            tp = np.sum((all_predictions == 1) & (all_labels == 1))
            fp = np.sum((all_predictions == 1) & (all_labels == 0))
            fn = np.sum((all_predictions == 0) & (all_labels == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            print(f'ç²¾ç¡®çŽ‡: {precision:.4f}')
            print(f'å¬å›žçŽ‡: {recall:.4f}')
            print(f'F1åˆ†æ•°: {f1:.4f}')
    
    return accuracy


def predict_similarity(model, img1_path, img2_path, transform, device='cuda'):
    """é¢„æµ‹ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦"""
    model.eval()
    
    # åŠ è½½å›¾ç‰‡
    img1 = Image.open(img1_path).convert('RGB')
    img2 = Image.open(img2_path).convert('RGB')
    
    # åº”ç”¨å˜æ¢
    img1 = transform(img1).unsqueeze(0).to(device)
    img2 = transform(img2).unsqueeze(0).to(device)
    
    with torch.no_grad():
        similarity_score, euclidean_dist, cosine_sim = model(img1, img2)
    
    return {
        'similarity_score': similarity_score.item(),
        'euclidean_distance': euclidean_dist.item(),
        'cosine_similarity': cosine_sim.item(),
        'is_similar': similarity_score.item() > 0.5
    }