#!/usr/bin/env python3


import matplotlib.pyplot as plt
import numpy as np
import re

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def parse_training_log(log_text):
    """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–æŸå¤±å’Œå‡†ç¡®ç‡æ•°æ®"""
    
    epochs = []
    train_losses = []
    val_accuracies = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®
    pattern = r'Epoch (\d+)/\d+:.*?train_loss=([\d.]+).*?val_acc=([\d.]+)'
    
    matches = re.findall(pattern, log_text)
    
    for match in matches:
        epoch = int(match[0])
        train_loss = float(match[1])
        val_acc = float(match[2])
        
        epochs.append(epoch)
        train_losses.append(train_loss)
        val_accuracies.append(val_acc)
    
    return epochs, train_losses, val_accuracies

def plot_training_curves(epochs, train_losses, val_accuracies):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç»˜åˆ¶è®­ç»ƒæŸå¤±
    ax1.plot(epochs, train_losses, 'b-', linewidth=2, marker='o', markersize=4, label='è®­ç»ƒæŸå¤±')
    ax1.set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)
    ax1.set_ylabel('æŸå¤±å€¼ (Loss)', fontsize=12)
    ax1.set_title('è®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend(fontsize=11)
    
    # è®¾ç½®yè½´èŒƒå›´ï¼Œè®©æ›²çº¿æ›´æ¸…æ™°
    ax1.set_ylim(0, max(train_losses) * 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i, (x, y) in enumerate(zip(epochs, train_losses)):
        if i % 3 == 0:  # æ¯3ä¸ªç‚¹æ ‡æ³¨ä¸€æ¬¡ï¼Œé¿å…è¿‡å¯†
            ax1.annotate(f'{y:.3f}', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontsize=8)
    
    # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡
    ax2.plot(epochs, val_accuracies, 'r-', linewidth=2, marker='s', markersize=4, label='éªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)
    ax2.set_ylabel('å‡†ç¡®ç‡', fontsize=12)
    ax2.set_title('éªŒè¯å‡†ç¡®ç‡å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend(fontsize=11)
    
    # è®¾ç½®yè½´èŒƒå›´
    ax2.set_ylim(min(val_accuracies) * 0.98, 1.0)
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i, (x, y) in enumerate(zip(epochs, val_accuracies)):
        if i % 3 == 0:  # æ¯3ä¸ªç‚¹æ ‡æ³¨ä¸€æ¬¡
            ax2.annotate(f'{y:.4f}', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontsize=8)
    
    # æ ‡è®°æœ€ä½³æ€§èƒ½ç‚¹
    best_idx = np.argmax(val_accuracies)
    best_epoch = epochs[best_idx]
    best_acc = val_accuracies[best_idx]
    
    ax2.scatter(best_epoch, best_acc, color='gold', s=100, zorder=5, 
               label=f'æœ€ä½³æ€§èƒ½: Epoch {best_epoch}, å‡†ç¡®ç‡ {best_acc:.4f}')
    ax2.legend(fontsize=11)
    
    plt.tight_layout()
    return fig

def main():
    """ä¸»å‡½æ•°"""
    
    # è®­ç»ƒæ—¥å¿—æ•°æ®
    log_text = """
    æ¨¡å‹æ€»å‚æ•°æ•°: 11,997,249
    å¯è®­ç»ƒå‚æ•°æ•°: 11,997,249
    å¼€å§‹è®­ç»ƒ...
    Epoch 1/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.4175 ETA: 00:00 19.8it/s
    Validating...
    Epoch 1/1000: train_loss=0.4175 val_acc=0.9486 lr=0.001000 time=56.0s
    ğŸ† New best model saved! Val Acc: 0.9486
    --------------------------------------------------------------------------------
    Epoch 2/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.2298 ETA: 00:00 21.5it/s
    Validating...
    Epoch 2/1000: train_loss=0.2298 val_acc=0.9651 lr=0.001000 time=52.2s
    ğŸ† New best model saved! Val Acc: 0.9651
    --------------------------------------------------------------------------------
    Epoch 3/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1913 ETA: 00:00 21.2it/s
    Validating...
    Epoch 3/1000: train_loss=0.1913 val_acc=0.9752 lr=0.001000 time=52.4s
    ğŸ† New best model saved! Val Acc: 0.9752
    --------------------------------------------------------------------------------
    Epoch 4/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1765 ETA: 00:00 21.6it/s
    Validating...
    Epoch 4/1000: train_loss=0.1765 val_acc=0.9656 lr=0.001000 time=51.6s
    --------------------------------------------------------------------------------
    Epoch 5/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1623 ETA: 00:00 20.9it/s
    Validating...
    Epoch 5/1000: train_loss=0.1623 val_acc=0.9781 lr=0.001000 time=54.0s
    ğŸ† New best model saved! Val Acc: 0.9781
    --------------------------------------------------------------------------------
    Epoch 6/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1466 ETA: 00:00 21.5it/s
    Validating...
    Epoch 6/1000: train_loss=0.1466 val_acc=0.9808 lr=0.001000 time=51.8s
    ğŸ† New best model saved! Val Acc: 0.9808
    --------------------------------------------------------------------------------
    Epoch 7/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1531 ETA: 00:00 21.7it/s
    Validating...
    Epoch 7/1000: train_loss=0.1531 val_acc=0.9824 lr=0.001000 time=51.4s
    ğŸ† New best model saved! Val Acc: 0.9824
    --------------------------------------------------------------------------------
    Epoch 8/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1330 ETA: 00:00 21.1it/s
    Validating...
    Epoch 8/1000: train_loss=0.1330 val_acc=0.9813 lr=0.001000 time=53.0s
    --------------------------------------------------------------------------------
    Epoch 9/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1386 ETA: 00:00 21.5it/s
    Validating...
    Epoch 9/1000: train_loss=0.1386 val_acc=0.9769 lr=0.001000 time=51.7s
    --------------------------------------------------------------------------------
    Epoch 10/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1278 ETA: 00:00 21.4it/s
    Validating...
    Epoch 10/1000: train_loss=0.1278 val_acc=0.9794 lr=0.001000 time=52.1s
    --------------------------------------------------------------------------------
    Epoch 11/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1251 ETA: 00:00 20.9it/s
    Validating...
    Epoch 11/1000: train_loss=0.1251 val_acc=0.9753 lr=0.001000 time=53.2s
    --------------------------------------------------------------------------------
    Epoch 12/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1220 ETA: 00:00 21.6it/s
    Validating...
    Epoch 12/1000: train_loss=0.1220 val_acc=0.9818 lr=0.001000 time=51.6s
    --------------------------------------------------------------------------------
    Epoch 13/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1174 ETA: 00:00 20.4it/s
    Validating...
    Epoch 13/1000: train_loss=0.1174 val_acc=0.9763 lr=0.001000 time=54.4s
    --------------------------------------------------------------------------------
    Epoch 14/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1152 ETA: 00:00 21.1it/s
    Validating...
    Epoch 14/1000: train_loss=0.1152 val_acc=0.9786 lr=0.001000 time=52.8s
    --------------------------------------------------------------------------------
    Epoch 15/1000 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [100.0%] loss: 0.1202 ETA: 00:00 20.3it/s
    Validating...
    Epoch 15/1000: train_loss=0.1202 val_acc=0.9805 lr=0.001000 time=54.5s
    """
    
    # è§£ææ—¥å¿—æ•°æ®
    epochs, train_losses, val_accuracies = parse_training_log(log_text)
    
    # ç»˜åˆ¶å›¾è¡¨
    fig = plot_training_curves(epochs, train_losses, val_accuracies)
    

    print(f"\nå›¾è¡¨å·²ä¿å­˜:")
    print(f"  - train.png (åŒ…å«è®­ç»ƒæŸå¤±å’ŒéªŒè¯å‡†ç¡®ç‡)")
    print(f"  - val.png (ç›¸åŒå†…å®¹çš„å‰¯æœ¬)")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()
    

if __name__ == "__main__":
    main()
